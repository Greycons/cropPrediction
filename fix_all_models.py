#!/usr/bin/env python3
"""
Comprehensive fix for all models to use consistent features
"""

import os
import pickle
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, BayesianRidge
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def create_consistent_dataset():
    """Create a dataset with consistent features that match the app input"""
    
    print("üîß Creating consistent dataset...")
    
    # Define the exact categorical values that will be used in the app
    states = ['Andhra Pradesh', 'Karnataka', 'Tamil Nadu', 'Maharashtra', 'Gujarat']
    districts = ['Anantapur', 'Bangalore', 'Chennai', 'Mumbai', 'Ahmedabad']
    crops = ['Rice', 'Wheat', 'Maize', 'Sugarcane', 'Cotton']
    
    # Generate synthetic data
    np.random.seed(42)
    n_samples = 2000
    
    data = {
        'state': np.random.choice(states, n_samples),
        'district': np.random.choice(districts, n_samples),
        'crop': np.random.choice(crops, n_samples),
        'year': np.random.choice(range(2015, 2025), n_samples),
        'groundwater_ph': np.random.normal(7.5, 0.5, n_samples),
        'ec_groundwater_(¬µs/cm)': np.random.normal(1200, 300, n_samples),
        'hardness_groundwater_(mg/l)': np.random.normal(400, 100, n_samples),
        'nitrate_groundwater_(mg/l)': np.random.normal(40, 20, n_samples),
        'rainfall_mm': np.random.normal(650, 150, n_samples),
        'soil_ph': np.random.normal(7.2, 0.4, n_samples),
        'soil_organic_carbon': np.random.normal(0.7, 0.1, n_samples),
        'soil_nitrogen': np.random.normal(200, 50, n_samples),
        'soil_phosphorus': np.random.normal(20, 5, n_samples),
        'soil_potassium': np.random.normal(250, 100, n_samples)
    }
    
    # Create DataFrame
    df = pd.DataFrame(data)
    
    # One-hot encode categorical variables (same as in app)
    df_encoded = pd.get_dummies(df, columns=['state', 'district', 'crop'], drop_first=True)
    
    # Create target variable with realistic relationships
    y = (2000 + 
         df_encoded['soil_nitrogen'] * 2 +
         df_encoded['soil_phosphorus'] * 10 +
         df_encoded['soil_potassium'] * 0.5 +
         df_encoded['rainfall_mm'] * 0.5 +
         np.random.normal(0, 200, n_samples))
    
    # Ensure positive yields
    y = np.abs(y)
    
    # Prepare features (exclude target)
    X = df_encoded.copy()
    
    print(f"üìä Generated {len(X)} samples with {X.shape[1]} features")
    print(f"üéØ Target range: {y.min():.1f} - {y.max():.1f}")
    print(f"üìã Feature columns: {list(X.columns)}")
    
    return X, y

def train_all_models(X, y):
    """Train all models with the same dataset"""
    
    print("\nü§ñ Training all models with consistent features...")
    
    # Create notebooks directory if it doesn't exist
    os.makedirs('notebooks', exist_ok=True)
    
    # Models to train
    models_to_create = {
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5),
        'Ridge': Ridge(alpha=1.0, random_state=42),
        'Bayesian Ridge': BayesianRidge(),
        'MLP': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    }
    
    trained_models = {}
    
    for name, model in models_to_create.items():
        try:
            print(f"ü§ñ Training {name}...")
            
            if name == 'MLP':
                # MLP needs scaling
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)
                model.fit(X_scaled, y)
                
                # Save model with scaler
                model_data = {'model': model, 'scaler': scaler}
                with open(f'notebooks/{name.lower().replace(" ", "_")}_model.pkl', 'wb') as f:
                    pickle.dump(model_data, f)
                trained_models[name] = model_data
            else:
                model.fit(X, y)
                with open(f'notebooks/{name.lower().replace(" ", "_")}_model.pkl', 'wb') as f:
                    pickle.dump(model, f)
                trained_models[name] = model
            
            # Test the model
            if name == 'MLP':
                X_test_scaled = scaler.transform(X[:5])
                pred = model.predict(X_test_scaled)
            else:
                pred = model.predict(X[:5])
            
            print(f"‚úÖ {name} model created successfully")
            print(f"   Sample prediction: {pred[0]:.2f}")
            print(f"   Feature count: {X.shape[1]}")
            
        except Exception as e:
            print(f"‚ùå Error creating {name}: {str(e)}")
    
    # Create XGBoost model (if available)
    try:
        import xgboost as xgb
        print("ü§ñ Training XGBoost...")
        xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, max_depth=5)
        xgb_model.fit(X, y)
        
        with open('notebooks/xgb_model.pkl', 'wb') as f:
            pickle.dump(xgb_model, f)
        trained_models['XGBoost'] = xgb_model
        
        pred = xgb_model.predict(X[:5])
        print(f"‚úÖ XGBoost model created successfully")
        print(f"   Sample prediction: {pred[0]:.2f}")
        print(f"   Feature count: {X.shape[1]}")
        
    except ImportError:
        print("‚ö†Ô∏è XGBoost not available, skipping...")
    
    # Create CatBoost model (if available)
    try:
        import catboost as cb
        print("ü§ñ Training CatBoost...")
        cat_model = cb.CatBoostRegressor(iterations=100, random_seed=42, verbose=False)
        cat_model.fit(X, y)
        
        with open('notebooks/catboost_model.pkl', 'wb') as f:
            pickle.dump(cat_model, f)
        trained_models['CatBoost'] = cat_model
        
        pred = cat_model.predict(X[:5])
        print(f"‚úÖ CatBoost model created successfully")
        print(f"   Sample prediction: {pred[0]:.2f}")
        print(f"   Feature count: {X.shape[1]}")
        
    except ImportError:
        print("‚ö†Ô∏è CatBoost not available, skipping...")
    
    # Create LightGBM model (if available)
    try:
        import lightgbm as lgb
        print("ü§ñ Training LightGBM...")
        lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
        lgb_model.fit(X, y)
        
        with open('notebooks/lgbm_model.pkl', 'wb') as f:
            pickle.dump(lgb_model, f)
        trained_models['LightGBM'] = lgb_model
        
        pred = lgb_model.predict(X[:5])
        print(f"‚úÖ LightGBM model created successfully")
        print(f"   Sample prediction: {pred[0]:.2f}")
        print(f"   Feature count: {X.shape[1]}")
        
    except ImportError:
        print("‚ö†Ô∏è LightGBM not available, skipping...")
    
    return trained_models, X.columns.tolist()

def save_feature_info(feature_columns):
    """Save feature information for the app to use"""
    
    feature_info = {
        'feature_columns': feature_columns,
        'n_features': len(feature_columns),
        'feature_types': {
            'categorical': ['state', 'district', 'crop'],
            'numerical': ['year', 'groundwater_ph', 'ec_groundwater_(¬µs/cm)', 
                         'hardness_groundwater_(mg/l)', 'nitrate_groundwater_(mg/l)', 
                         'rainfall_mm', 'soil_ph', 'soil_organic_carbon', 
                         'soil_nitrogen', 'soil_phosphorus', 'soil_potassium']
        }
    }
    
    with open('notebooks/feature_info.pkl', 'wb') as f:
        pickle.dump(feature_info, f)
    
    print(f"üíæ Feature info saved: {len(feature_columns)} features")

def test_prediction_consistency(models, feature_columns):
    """Test that all models can make predictions with the same input"""
    
    print("\nüß™ Testing prediction consistency...")
    
    # Create test input (same format as app)
    test_input = {
        'state': 'Andhra Pradesh',
        'district': 'Anantapur',
        'year': 2024,
        'crop': 'Rice',
        'groundwater_ph': 7.5,
        'ec_groundwater_(¬µs/cm)': 1200,
        'hardness_groundwater_(mg/l)': 400,
        'nitrate_groundwater_(mg/l)': 40,
        'rainfall_mm': 650,
        'soil_ph': 7.2,
        'soil_organic_carbon': 0.7,
        'soil_nitrogen': 200,
        'soil_phosphorus': 20,
        'soil_potassium': 250
    }
    
    # Create DataFrame and encode
    input_df = pd.DataFrame([test_input])
    input_encoded = pd.get_dummies(input_df, columns=['state', 'district', 'crop'], drop_first=True)
    
    # Align with expected features
    for col in feature_columns:
        if col not in input_encoded.columns:
            input_encoded[col] = 0
    
    input_encoded = input_encoded[feature_columns]
    
    print(f"üìä Test input has {input_encoded.shape[1]} features")
    
    # Test each model
    predictions = {}
    for name, model in models.items():
        try:
            if isinstance(model, dict) and 'model' in model:
                # Handle models with scalers
                scaler = model.get('scaler')
                model_obj = model['model']
                if scaler and model_obj:
                    input_scaled = scaler.transform(input_encoded)
                    pred = model_obj.predict(input_scaled)[0]
                else:
                    pred = model_obj.predict(input_encoded)[0]
            else:
                pred = model.predict(input_encoded)[0]
            predictions[name] = pred
            print(f"‚úÖ {name}: {pred:.2f}")
        except Exception as e:
            print(f"‚ùå {name}: {str(e)}")
    
    if predictions:
        avg_pred = np.mean(list(predictions.values()))
        print(f"\nüéØ Average prediction: {avg_pred:.2f}")
        print(f"üìä Successful models: {len(predictions)}/{len(models)}")
        return True
    else:
        print("\n‚ùå No successful predictions")
        return False

def main():
    print("üîß Comprehensive Model Fix")
    print("=" * 50)
    
    # Create consistent dataset
    X, y = create_consistent_dataset()
    
    # Train all models
    models, feature_columns = train_all_models(X, y)
    
    # Save feature info
    save_feature_info(feature_columns)
    
    # Test consistency
    success = test_prediction_consistency(models, feature_columns)
    
    if success:
        print("\nüéâ All models are now consistent!")
        print("üöÄ Parameter prediction should work in the Streamlit app")
    else:
        print("\n‚ùå Some models still have issues")
    
    print(f"\nüìÅ Created model files:")
    for file in os.listdir('notebooks'):
        if file.endswith('_model.pkl'):
            print(f"   - {file}")

if __name__ == "__main__":
    main()
